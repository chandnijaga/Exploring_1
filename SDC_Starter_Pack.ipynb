{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------- Sentosa Starter Pack ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "* [0. Initial Setup](#0.InitialSetup)\n",
    "\n",
    "* [1. Basic-Utils](#1.Basic-Utils)\n",
    "\n",
    "* [2. Sentosa-Utils](#2.Sentosa-Utils)\n",
    "\n",
    "* [3. Machine Learning-Utils](#3.MachineLearning-Utils)\n",
    "\n",
    "* [4. How to use the utils](#4.Launcher)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Initial Setup <a class=\"anchor\" id=\"0.InitialSetup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "import pylab\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "import descartes\n",
    "import folium\n",
    "\n",
    "#Machine learning packages\n",
    "import statistics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={       \n",
    "   \n",
    "    \"TIDY_DATES\": ['Open Timestamp'], #list of columns to be formatted into datetime data types\n",
    "        \n",
    "    \n",
    "    \n",
    "    \"TARGET_VAR\": \"\", #specify what you are predicting\n",
    "    \"ID_VAR\": \"\", #specify the ID variable which doesnot have to be included in the model\n",
    "    \"DROP_LIST\": [] #specify list of columns from data frame which doesnot have to be included in model training\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic-Utils <a class=\"anchor\" id=\"1.Basic-Utils\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tidy_dates(df, config):    \n",
    "    \"\"\"\n",
    "    Format the dates to specific format\n",
    "    Inputs      : df     = dataframe where dates have to be formatted\n",
    "                : config = list of date columns\n",
    "    outputs     : model data frame with formatted date columns\n",
    "    \"\"\"\n",
    "    \n",
    "    df[config['TIDY_DATES']]=df[config['TIDY_DATES']].apply(pd.to_datetime,format='%d/%m/%Y %p %I:%M:%S')\n",
    "        \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(df,X,HUE=None,TITLE=None):\n",
    "    \"\"\"\n",
    "    Plot histogram of a variable from a given dataframe\n",
    "    Inputs      : df     = dataframe where the column to be plotted exists\n",
    "                : X      = column for which histogram has to be plotted\n",
    "                : HUE    = [Optional] category column to separate various histogram plots\n",
    "                : TITLE  = [Optional] Title of the chart\n",
    "    outputs     : Histogram plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.histplot(data=df, x=X, hue=HUE,element='step')\n",
    "    plt.title(TITLE, fontsize=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(df,X,Y,HUE=None,TITLE=None):\n",
    "    \"\"\"\n",
    "    Plot line chart of a variable from a given dataframe\n",
    "    Inputs      : df     = dataframe where the column to be plotted exists\n",
    "                : X      = column name for X-axis, generally the date column\n",
    "                : Y      = column name to be plotted in the y-axis\n",
    "                : HUE    = [Optional] category column to separate various line plots\n",
    "                : TITLE  = [Optional] Title of the chart\n",
    "    outputs     : Line plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.lineplot(data=df, x=X,y=Y, hue=HUE)\n",
    "    plt.title(TITLE, fontsize=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bar(df,X,HUE=None,TITLE=None,):\n",
    "    \"\"\"\n",
    "    Plot bar chart of a variable from a given dataframe\n",
    "    Inputs      : df     = dataframe where the column to be plotted exists\n",
    "                : X      = column for which bar chart has to be plotted\n",
    "                : HUE    = [Optional] category column to separate various bar plots\n",
    "                : TITLE  = [Optional] Title of the chart\n",
    "    outputs     : Bar plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.countplot(data=df, x=X, hue=HUE, color='Salmon')\n",
    "    plt.title(TITLE, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(x,y):\n",
    "    \"\"\"\n",
    "    Plot histogram of a variable from a given dataframe\n",
    "    Inputs      : x      = values to be plotted in X-axis\n",
    "                : y      = values to be plotted in Y-axis\n",
    "    outputs     : Scatter plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.scatter(x,y)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentosa-Utils <a class=\"anchor\" id=\"2.Sentosa-Utils\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_of_day(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create time of the day for the date column\n",
    "    Inputs      : df      = sdc_df, bus ridership data\n",
    "    outputs     : df, updated data frame with a new column called \"TIME_OF_DAY\"\n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    df['TIME_OF_DAY']=np.where(((df['Open Timestamp'].dt.hour>=16) & (df['Open Timestamp'].dt.hour<23)),\"Evening\",\n",
    "                            np.where(((df['Open Timestamp'].dt.hour>=7) & (df['Open Timestamp'].dt.hour<12)),\"Morning\",\n",
    "                                            np.where(((df['Open Timestamp'].dt.hour>=12) & (df['Open Timestamp'].dt.hour<16)),\"Afternoon\",\n",
    "                                                    \"Night\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone_attraction_df(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create time of the day for the date column\n",
    "    Inputs      : df      = attraction_df, list of all attractions in the data provided such as F&B, attraction, etc.\n",
    "    outputs     : zone_df, updated data frame total attraction summarized and score normalized between 1-10\n",
    "    \"\"\"   \n",
    "    \n",
    "    zone_df=df.groupby(by='Zone')[['Lat','Long']].mean().reset_index()\n",
    "\n",
    "    zone_attraction_df=df[df.Sector.isin(['Attraction','F&B','Hotel','Transport'])].pivot_table(index=['Zone'],columns='Sector',values='Places/ Location',aggfunc='count',fill_value=0).reset_index()\n",
    "    zone_attraction_df['Total_attraction']=zone_attraction_df['Attraction']*0.4+zone_attraction_df['F&B']*0.25+zone_attraction_df['Hotel']*0.25+zone_attraction_df['Transport']*0.10\n",
    "\n",
    "    zone_df=pd.merge(zone_df,zone_attraction_df, on='Zone', how='left')\n",
    "    zone_df['Total_attraction']=zone_df['Total_attraction'].fillna(0)\n",
    "    \n",
    "    zone_df['Total_attraction_Normalized']=(zone_df['Total_attraction']-zone_df['Total_attraction'].min())/(zone_df['Total_attraction'].max()-zone_df['Total_attraction'].min())*(10-1)+1\n",
    "    \n",
    "    return zone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone_map(df): \n",
    "    \n",
    "    \"\"\"\n",
    "    Plot map of sentosa based on the attractions/zone data frame\n",
    "    Inputs      : df      = zone_df, zone or attractions data frame\n",
    "    outputs     : map of sentosa with all zone plotted and size of zone highlights the number of attractions in respective zones\n",
    "    \"\"\"   \n",
    "\n",
    "    lat=df.Lat\n",
    "    long=df.Long\n",
    "\n",
    "    zone_map = folium.Map(location=[1.249, 103.83], zoom_start=14.458)\n",
    "    for i in range(len(lat)):\n",
    "        folium.Circle( location=[ lat[i], long[i] ], popup=df.Zone[i], radius=df.Total_attraction[i]*15, color='darkgreen', fill=True, fill_color='darkgreen').add_to( zone_map )\n",
    "\n",
    "    #zone_map.save('map.html')\n",
    "    return zone_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bus_zone_distance(df1,df2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate distance in metres between two lat and long positions\n",
    "    Inputs      : df1      = bus_df,containing list of unique bus stops and lat and long\n",
    "                : df2      = zone_df, the data frame created above with list of all zones and their lat and long\n",
    "    outputs     : updated df1 with the closest zone to the bus stop marked\n",
    "    \"\"\"  \n",
    "    \n",
    "    R = 6371\n",
    "    \n",
    "    for i in range(len(df2)):\n",
    "\n",
    "        dlat=np.radians(df1.Lat - df2.Lat[i])\n",
    "        dlong=np.radians(df1.Long - df2.Long[i])\n",
    "\n",
    "        a = np.sin(dlat / 2)**2 + np.cos(np.radians(df2.Lat[i])) * np.cos(np.radians(df1.Lat)) * np.sin(dlong / 2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        distance = R * c    \n",
    "        df1[df2.Zone[i]]=np.round(distance,3)*1000\n",
    "        \n",
    "    df1['Zone'] = df1[df2.Zone.unique()].idxmin(axis=1)\n",
    "    df1=pd.merge(df1,df2[['Zone','Attraction','F&B','Total_attraction','Total_attraction_Normalized']], on='Zone', how='left')\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attractiveness_factor_busstop(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    get popularity factor for each bus stops based on the closeby attractions to the bus stop and guests demand at the stop\n",
    "    Inputs      : df      = bus-df with list of stops and merged with total guests demand in the total data provided\n",
    "    outputs     : updated data frame with a new column of \"Popularity_factor\"\n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    df['Total_Guests_Normalized']=((df.Total_Guests-df.Total_Guests.min())/(df.Total_Guests.max()-df.Total_Guests.min()))*(10-1)+1\n",
    "    \n",
    "    df['Popularity_factor']=0.6*df['Total_Guests_Normalized']+0.4*df['Total_attraction_Normalized']\n",
    "    \n",
    "    df['Popularity_factor']=((df.Popularity_factor-df.Popularity_factor.min())/(df.Popularity_factor.max()-df.Popularity_factor.min()))*(10-1)+1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_related_features(df):\n",
    "    \"\"\"\n",
    "    Get a dataframe creating all time/seasonality related features on a given date column\n",
    "    Inputs      : df      = Sentosa dataframe \n",
    "    outputs     : Updated data frame with all new time/seasonality features created\n",
    "    \"\"\"\n",
    "    \n",
    "    #time seasionality    \n",
    "    df['MONTH_OF_YEAR']=df['Open Timestamp'].dt.month\n",
    "    df['DAY_OF_MONTH']=df['Open Timestamp'].dt.day\n",
    "    df['WEEK_OF_YEAR']=df['Open Timestamp'].dt.isocalendar().week\n",
    "    df['YEAR']=df['Open Timestamp'].dt.year\n",
    "    df['QUARTER_OF_YEAR']=df['Open Timestamp'].dt.quarter\n",
    "    df['IS_YEAR_START']=np.where(df.MONTH_OF_YEAR<3,1,0)\n",
    "    df['IS_YEAR_END']=np.where(df.MONTH_OF_YEAR>10,1,0)\n",
    "    \n",
    "    df['TEMP_TIME']=df['Open Timestamp'].dt.hour\n",
    "    df['TIME_OF_DAY']=np.where((df.TEMP_TIME>=22) & (df.TEMP_TIME<6),\"Night\",np.where((df.TEMP_TIME>=6) & (df.TEMP_TIME<14),\"Morning\",\"Evening\"))\n",
    "    \n",
    "    df=df.drop('TEMP_TIME', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 For Network Maps of Bus and Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_nodes_edges(bus_df,sdc_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create unique nodes and edges from the bus data frame; nodes represent all bus stops and edges represent # guests transferring between two bus stops\n",
    "    Inputs      : bus_df      = Unique list of all bust stops\n",
    "                : sdc_df      = sentosa ridership data frame\n",
    "    outputs     : Nodes and edges data frame\n",
    "    \"\"\"\n",
    "\n",
    "    nodes_df=(bus_df[['Bus Stops','Popularity_factor']].drop_duplicates()).copy()\n",
    "\n",
    "    edge_df=sdc_df[sdc_df['Total_Guests']>=1].groupby(by=['Bus Stop','next_stop'])['Total_Guests'].sum().reset_index()\n",
    "    edge_df=edge_df.rename(columns={'Bus Stop':'from','next_stop':'to','Total_Guests':'weight'})\n",
    "    edge_df=edge_df[~(edge_df['from']==edge_df['to'])]\n",
    "    \n",
    "    edge_df['weight']=(edge_df.weight-edge_df.weight.min())/(edge_df.weight.max()-edge_df.weight.min())*10\n",
    "    \n",
    "\n",
    "    return nodes_df, edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_dict(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get a dictionary for each node i.e., the bus stop and their lat and long. The dictionary will be used to plot the bus stops on the network map\n",
    "    Inputs      : df          = a dataframe with list of unique bus stops and their lat and long columns                \n",
    "    outputs     : Dictionary with nodes i.e., bus stop names and their lat/long\n",
    "    \"\"\"\n",
    "    \n",
    "    routes_lats=df[['Bus Stops','Long','Lat']]\n",
    "    xy=routes_lats.drop_duplicates().copy()\n",
    "    xy['pos']=list(zip(xy.Long,xy.Lat))\n",
    "    routes_lats=xy[['Bus Stops','pos']]\n",
    "    routes_lats=routes_lats.set_index('Bus Stops')\n",
    "    routes_lats=routes_lats.to_dict('index')\n",
    "    pos_dict={}\n",
    "\n",
    "\n",
    "    for key,value in routes_lats.items():\n",
    "        for key2,value2 in value.items():\n",
    "            pos_dict[key] = np.asarray(value2)\n",
    "            \n",
    "    return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_graph(nodes_df,edge_df,pos_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot network graph based on the nodes and edges data frame created. Also use the pos_dict to specify where to plot the nodes based on their lat and long\n",
    "    Inputs      : nodes_df      = Unique list of all bust stops\n",
    "                : edge_df       = sentosa ridership data frame\n",
    "                : pos_dict      =\n",
    "    outputs     : netwrok map of bus stops(nodes) and guests travelling between stops (edges)\n",
    "    \"\"\"\n",
    "\n",
    "    G=nx.from_pandas_edgelist(edge_df, 'from', 'to', edge_attr='weight', create_using=nx.DiGraph() )\n",
    "    widths = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "    plt.figure(figsize=(30,30))\n",
    "    \n",
    "    #pos = nx.circular_layout(G)\n",
    "    pos=pos_dict\n",
    "    nx.draw_networkx_nodes(G,pos,\n",
    "                           nodelist=nodes_df['Bus Stops'],\n",
    "                           node_size=nodes_df['Popularity_factor']*10**3,\n",
    "                           cmap=plt.cm.Greys,\n",
    "                           #node_color='#41554F',\n",
    "                           alpha=0.4)\n",
    "    nx.draw_networkx_edges(G,pos,\n",
    "                           edgelist = widths.keys(),\n",
    "                           width=list(widths.values()),\n",
    "                           edge_color='dimgray',\n",
    "                           alpha=1)\n",
    "    nx.draw_networkx_labels(G, pos=pos,\n",
    "                            labels=dict(zip(nodes_df['Bus Stops'],nodes_df['Bus Stops'])),\n",
    "                            font_color='black', font_size=14)\n",
    "    plt.box(False)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_busstops_guests_hourly(df):    \n",
    "    \n",
    "    \"\"\"\n",
    "    Create a dataframe which summarizes the total guests visiting each bus stop in a given route every hour\n",
    "    Inputs      : df          = sdc_df i.e., bus ridership cleaned data\n",
    "    outputs     : df_hourly, new data frame with hourly level total guests at a given bus stop in a given route \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    df_hourly=sdc_df[( (df.TIME_OF_DAY.isin(['Morning','Afternoon','Evening'])))\n",
    "                    ].groupby([pd.Grouper(key='Open Timestamp', freq='H'), \n",
    "                                pd.Grouper(key='Route'),\n",
    "                                   pd.Grouper(key='Bus Stop'),\n",
    "                                      pd.Grouper(key='WEEKEND'),])[['Total_Guests']].sum().reset_index()\n",
    "\n",
    "    df_hourly['HOUR_OF_DAY']=df_hourly['Open Timestamp'].dt.hour\n",
    "    \n",
    "    return df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_busstops_guests_hourly_distribution(df, bus_route, bus_stop, weekend): \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Plot hourly distribution of total guests at a bus stop and a bus route\n",
    "    Inputs      : df          = df_hourly, new data frame with hourly level total guests at a given bus stop in a given route\n",
    "                : bus_route   = specify the bus route A,B or C\n",
    "                : bus_stop    = specify from the list of available bus stops in Sentosa\n",
    "                : weekend     = flag of 0= weekday and 1= weekend\n",
    "    outputs     : Hourly distribution plot of total guests at a bus stop\n",
    "    \"\"\"\n",
    "\n",
    "    #hourly distribution for bus in all routes and all time of day\n",
    "    df_hourly_distribution=df.groupby(by=['Route','Bus Stop','WEEKEND','HOUR_OF_DAY'])['Total_Guests'].agg(['mean','count','std','min','max']).reset_index()\n",
    "    df_hourly_distribution['lower_bound']=round(df_hourly_distribution['mean']-(1.96*(df_hourly_distribution['std']/((df_hourly_distribution['count'])**1/2))),2)\n",
    "    df_hourly_distribution['upper_bound']=round(df_hourly_distribution['mean']+(1.96*(df_hourly_distribution['std']/((df_hourly_distribution['count'])**1/2))),2)\n",
    "   \n",
    "    \n",
    "    #hourly distribution plot for a specific bus stop in a given route\n",
    "    fig, axes = plt.subplots(ncols=5, nrows=3, figsize=(15,8))\n",
    "    \n",
    "    for i, ax in zip(range(7,22), axes.flat):\n",
    "        sns.histplot((df[(df['Route']==bus_route) & (df['Bus Stop']==bus_stop) &\n",
    "                         (df['WEEKEND']==weekend) & (df.HOUR_OF_DAY==i)]['Total_Guests']), \n",
    "                             ax=ax, kde=True).set_title(bus_stop + \" - Hour of day: \"+ str(i) +\":00\")\n",
    "        \n",
    "        max_lim=df_hourly_distribution[(df_hourly_distribution['Route']==bus_route) & ( df_hourly_distribution['Bus Stop']==bus_stop)]['max'].max()\n",
    "        ax.set_xlim(0,max_lim)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_hourly_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning-Utils <a class=\"anchor\" id=\"3.MachineLearning-Utils\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(model_df, config):\n",
    "  \"\"\"\n",
    "  Drop the list of unnecessary features not to be used for prediction purpose\n",
    "  Inputs      : df     = dataframe that columns need to be dropped\n",
    "              : config = list of columns to be dropped\n",
    "  outputs     : model data frame with dropped columns\n",
    "  \"\"\"\n",
    "  for i in range(0, len(config['DROP_LIST'])):\n",
    "    try: \n",
    "      model_df.drop(config['DROP_LIST'][i], axis=1, inplace=True)\n",
    "    except:\n",
    "      model_df = model_df.copy()\n",
    "      \n",
    "  return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(model_df,config):\n",
    "  \"\"\"\n",
    "  Read the latest generated model data as part of the ML pipeline\n",
    "  Inputs      : df     = dataframe from which predictor and target variables can be extracted\n",
    "              : config = list of ID columns (unique identifier) and name of target variable\n",
    "  outputs     : two data frames with preditor and target variable separately\n",
    "  \"\"\"\n",
    "  X=model_df.drop(config['TARGET_VAR']+config['ID_VAR'], axis=1)\n",
    "  y=model_df[config['TARGET_VAR']]\n",
    "  return X,y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(X,y,config):\n",
    "  \"\"\"\n",
    "  Split the data into test and training set sequentially\n",
    "  Inputs      : X         = dataframe with all the predictor variables\n",
    "              : y         = dataframe with the target variable\n",
    "              : test_size = % of test data to be split \n",
    "  outputs     : 4 data frames consiting of train and test data separately\n",
    "  \"\"\"\n",
    "  X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, shuffle=False) \n",
    "  \n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_data(X,y,X_train, X_test, y_train, y_test):  \n",
    "    \"\"\"\n",
    "    Get scaled data useful for training predictive models\n",
    "    Inputs      : X       = predictor/independent variables \n",
    "                : y       = target/dependent variable\n",
    "                : X_train = training dataset for predictor variables\n",
    "                : X_test  = test dataset for predictor variables\n",
    "                : y_train = training dataset for target variable\n",
    "                : y_test  = test dataset for predictor variables\n",
    "                \n",
    "    outputs     : scaler definition for both train and test data set along-with scaled variables\n",
    "    \"\"\"\n",
    "        \n",
    "    scalerX = StandardScaler().fit(X)    \n",
    "    scalery = StandardScaler().fit(y)\n",
    "    \n",
    "    X_train_scaled = pd.DataFrame(scalerX.transform(X_train),columns = X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(scalerX.transform(X_test),columns = X_test.columns)\n",
    "    \n",
    "    y_train_scaled = pd.DataFrame(scalery.transform(y_train),columns = y_train.columns)\n",
    "    y_test_scaled = pd.DataFrame(scalery.transform(y_test),columns = y_test.columns)\n",
    "    \n",
    "    return scalerX, scalery, X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_transform(scalery,y_pred_scaled):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get scaled prediction data back to the original values \n",
    "    Inputs      : scalery       = scalar definition of y(dependent variable) dataset, obtained from 'get_scaled_data' function\n",
    "                : y_pred_scaled = y_pred value which has to be scaled back to normal\n",
    "    outputs     : updated predictions based on actual range\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = scalery.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(X_train,y_train):\n",
    "    \"\"\"\n",
    "    Train a regression model\n",
    "    Inputs      : X_train      = training data set only independent variables \n",
    "                : y_train      = training data set only target variable\n",
    "    outputs     : trained model file which can be used to predic on test data and also be saved as pickle file\n",
    "    \"\"\"\n",
    "           \n",
    "    model=LinearRegression().fit(X_train,y_train)       \n",
    "   \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_param_grid():\n",
    "    \"\"\"    \n",
    "    [Optional Function]\n",
    "    Parameter grid for hyperparameter tuning the model for tree based model\n",
    "    \"\"\"\n",
    "    n_estimators = [int(x) for x in np.linspace(start=200, stop=300, num=2)]\n",
    "    min_samples_split = [int(x) for x in np.linspace(start=100, stop=150, num=1)]\n",
    "    min_samples_leaf = [int(x) for x in np.linspace(start=60, stop=150, num=1)]\n",
    "    max_depth = [int(x) for x in np.linspace(start=10, stop=15, num=2)]\n",
    "    max_features = ['sqrt']\n",
    "    subsample = [x for x in np.linspace(start=0.5, stop=1.0, num=1)]\n",
    "\n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                  'max_depth': max_depth,\n",
    "                  'learning_rate': [0.1],\n",
    "                  'min_samples_split': min_samples_split,\n",
    "                  'min_samples_leaf': min_samples_leaf,\n",
    "                  'max_features': max_features,\n",
    "                  'subsample': subsample\n",
    "                }\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(X_train,y_train,param_grid):\n",
    "    \"\"\"\n",
    "    Model hyperparameter tuning: Iterate through grid search cv parameters to\n",
    "    find the best fit for the regressor model\n",
    "\n",
    "    Inputs: X_train    = matrix with list of predictor variables to be used in the\n",
    "                         predictive model\n",
    "          : y_train    = target variable column\n",
    "          : param_grid = hyperparameter tuning grid search cv initialization\n",
    "          : config\n",
    "\n",
    "    Output: model      = trained model with the best accuracy obtained from GridSearch CV\n",
    "    \"\"\"\n",
    "   \n",
    "    gridsearch = GridSearchCV(GradientBoostingClassifier(),\n",
    "                              param_grid=param_grid, n_jobs=-1)\n",
    "    \n",
    "    model = gridsearch.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "   \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model):  \n",
    "    \n",
    "    \"\"\"    \n",
    "    [Optional Function only for tree based models]\n",
    "    Get feature importance for the trained tree based model\n",
    "    \"\"\"\n",
    "\n",
    "    variable_importance=(pd.DataFrame(model.best_estimator_.feature_importances_, index=X.columns))\n",
    "\n",
    "    return variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model,X_test):\n",
    "    \"\"\"\n",
    "    [Optional Function only for tree based models]\n",
    "    Get predicted values for test data set using a trained model file\n",
    "    Inputs      : model        = trained model file \n",
    "                : X_test       = test data set of independent variables for prediction purpose\n",
    "    outputs     : final predicted values\n",
    "    \"\"\"         \n",
    "    \n",
    "    y_pred=model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_statistics(y_test,y_pred):\n",
    "    \"\"\"\n",
    "    [Optional Function only for regression models]\n",
    "    Get model stats such as MSE, MAPE, etc. for the model based on actual vs predicted values on test data set\n",
    "    Inputs      : y_test        = actual target variables values from test data set\n",
    "                : y_test        = predicted target variables values from test data set\n",
    "    outputs     : Model stats of actual vs predicted values\n",
    "    \"\"\"     \n",
    "    #generating various regression model evaluation stats  \n",
    "    mse=mean_squared_error(y_test, y_pred)  \n",
    "    mae=mean_absolute_error(y_test, y_pred)\n",
    "    mape=np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "    #Appending the results to a dataframe\n",
    "    model_results=[['MSE',mse],['MAE',mae],['MAPE',mape]]\n",
    "    model_statistics_df=pd.DataFrame(model_results,columns=['Measure', 'Value'])\n",
    "\n",
    "    return model_statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_metrics(y_test,y_pred):    \n",
    "    \"\"\"\n",
    "    [Optional Function only for classification models]\n",
    "    Get confusion matrix for the model based on actual vs predicted values on test data set\n",
    "    Inputs      : y_test        = actual target variables values from test data set\n",
    "                : y_test        = predicted target variables values from test data set\n",
    "    outputs     : confusion matrix of actual vs predicted values\n",
    "    \"\"\"     \n",
    "    \n",
    "    confusion_matrix=(pd.crosstab(y_test, y_pred, rownames=['Predicted'], colnames=['Actual'], normalize='columns'))*100\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    sns.heatmap(confusion_matrix, annot=True,  fmt='g', cmap='Blues',linewidths=1, linecolor='lightgrey')\n",
    "    \n",
    "    \n",
    "    return confusion_matrix\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    [Optional Function only for classification models]\n",
    "    Get classification report for the model based on actual vs predicted values on test data set\n",
    "    Inputs      : y_test        = actual target variables values from test data set\n",
    "                : y_test        = predicted target variables values from test data set\n",
    "    outputs     : classificatio report (precision/recall, etc.) of actual vs predicted values\n",
    "    \"\"\"   \n",
    "    \n",
    "    print((metrics.classification_report(y_test, y_pred, digits=2)))  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to use the utils <a class=\"anchor\" id=\"4.Launcher\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------NON EXHAUSTIVE USAGE LIST---------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------------READ DATA------------------------------------------------------------\n",
    "\n",
    "sdc_df=pd.read_csv(\"Bus_Ridership_Jan-Mar21_type2.csv\")\n",
    "\n",
    "#read list of buses in Sentosa and routes and sequence of stops in the route\n",
    "bus_df=pd.read_csv(\"bus_list.csv\")\n",
    "\n",
    "\n",
    "#---------------------------------------------------------PROCESS DATA---------------------------------------------------------\n",
    "\n",
    "#Use \"get_tidy_dates\" function to convert data into proper format; \n",
    "#do update the config file with list of columns which has to converted to datetime format\n",
    "sdc_df=get_tidy_dates(sdc_df, config)\n",
    "\n",
    "sdc_df=get_time_of_day(sdc_df)\n",
    "\n",
    "\n",
    "#------------------------------------------------------- Visualization/Heuristics Model ---------------------------------------\n",
    "\n",
    "#create hourly level guests data frame\n",
    "df_hourly=get_busstops_guests_hourly(sdc_df)\n",
    "#plot hourly guests distribution for the bus stop specified\n",
    "df_hourly_distribution=get_busstops_guests_hourly_distribution(df_hourly, 'Bus Route A', 'Siloso Point',weekend=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------ML Model Build----------------------------------------------------------\n",
    "\n",
    "#use \"prepare_model_data\" to divided dataframe into dependent and independent variables\n",
    "X,y=prepare_model_data(model_df,config)\n",
    "\n",
    "#use \"train_test_data\" function to split entire data set into training and testing\n",
    "X_train, X_test, y_train, y_test=train_test_data(X,y,config)\n",
    "\n",
    "#use \"train_regression_model\" to build a model using the training data set created above\n",
    "model=train_regression_model(X_train,y_train)\n",
    "\n",
    "#use \"get_prediction\" function to use the trained model to predict on test data set\n",
    "y_pred=get_prediction(model,X_test)\n",
    "\n",
    "#use \"get_model_statistics\" function to test model accuracy using metrics such as MAPE, MSE for regression based model\n",
    "model_statistics_df=get_model_statistics(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
